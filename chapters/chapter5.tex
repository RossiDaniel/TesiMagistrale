\hypertarget{(chap:capitolo6)}{}
\chapter{Libreria cornac e esperimenti}
In questo capitolo  andremo a vedere le modalità con cui sono stati svolti gli esperimenti e la libreria utilizzata per gli stessi.
\section{Esperimenti sulle matrici grezze}
\subsection{Dataset}
Le informazioni dello storico vendite sono state organizzate basandosi sulle due macrocategorie individuate nell'analisi: macchine e ricambi.
Gli item considerati sono quelli acquistati almeno una volta mentre per i clienti quelli che hanno effettuato almeno un'acquisto.
Abbiamo quindi tre tipi matrici grezze user-item contenenti solo item appartenenti alle macrocategoria delle macchine, solo dei ricambi ed infine una con tutti gli item detta totale.
In generale le divisione rispetto i tipi di dataset sono le seguenti:
\begin{itemize}
    \item \textbf{Macchine}: 254 user e 518 item;
    \item \textbf{Ricambi}: 319 user e 7699 item;
    \item \textbf{Totale}: 322 user e 8217 item;
\end{itemize}

Per ognuno di questi tipi di matrice abbiamo le quattro versioni delle espressioni di interesse, che sono state calcolate tra uno user $u$ e un item $i$ come segue:
\begin{itemize}
    \item \textbf{Quantità}: somma dei campi quantità (\textit{KWMENG}) di tutte le posizione delle fatture di $u$ in cui è presente $i$.
    \item \textbf{Spesa totale}: somma dei campi spesa totale (\textit{NETWR}) di tutte le posizione delle fatture di $u$ in cui è presente $i$.
    \item \textbf{Numero di fatture}: conta del numero di fatture di $u$ in cui compare $i$.
    \item \textbf{Recentezza}: ricerca della posizione di $i$ nelle fatture di $u$, riportante la data più recente di acquisto. Se l'item è stato acquistato avremo quindi una data a cui andremo a sottrarre la data della fattura più vecchia, il delta temporale viene trasformato in giorni. Quindi più è alto il delta in giorni più recente sarà l'acquisto.
\end{itemize}

\subsection{Libreria Cornac}
La libreria cornac gestisce completamente gli esperimenti, dall'acquisizione dei dati fino alla verifica dei risultati.
Nello specifico è stata scelta per la presenza di molti \textit{modelli} e metriche per la loro valutazione.\\
Nello specifico i modelli utilizzati sono stati:
\begin{itemize}
    \item MostPop: modello basato sulla popolarità, dove un item è più popolare in base al numero di user che lo hanno valutato, usato per ottenere un risultato di base;
    \item UserKnn: implementazione dell'approccio del collaborative filtering memory-based, che di default prende in considerazione i 20 user più simili ad uno target.
    \item ItemKnn: come il precedente, ma basato sugli item. Non è stato infine utilizzato in quanto durante le fasi iniziali di test non ha mai riportato risultati superiori a quelli di base e richiedeva troppo tempo per la valutazione;
    \item MF: implementazione del matrix factorization, metodo model-based del collaborative filtering, di default considera 10 user più simili ad uno target;
    \item VAECF: modello per la versione implicita, usato per avere un risultato di base.
\end{itemize}
I dati presi in input sono nel formato \textit{user, item, rating} e volendo potevano essere divisi in training, validation e test set direttamente dalla libreria.
Si è preferito però dividerli esternamente con le rispettive percentuali: 70\% al training set, 15\% per validation e test set.
Nella fase di valutazione dei risultati dei \textit{modelli} veniva richiesto un parametro detto \textit{rating\_threshold}, il quale serviva a binarizzare gli item rilevanti e irrilevanti del test set basandosi sul rating corrispondente. Non andava in alcun modo ad intaccare i rating nella fare di training.
 
\subsection{Esperimenti sulle singole matrici grezze}
Come riportato nel capitolo relativo il preprocessing delle matrici grezze, non è stata definita una scala dei rating in quanto se ne volevano provare diverse: $scale \in \{3, 5, 7, 9, 13, 19, 25\}$.
Inoltre per ogni tecnica dove fosse previsto l'utilizzo di una delle distribuzioni disponibili, si è proceduto a testarle entrambe.
Dalle combinazioni di: scala, distribuzione dei rating, espressione di interesse e dataset, si sono ottenute circa 1500 matrici dei rating.
Per eseguire i test si è proceduti come segue:
\begin{enumerate}
    \item Ottengo un risultato di base con il modello MostPop;
    \item Poi per ogni tecnica testo ciascuna combinazione di matrice dei rating, scala e funzione di distribuzione se usata, si eseguono le seguenti operazioni:
    Poi per ciascuna tecnica si procede a testare tutte le matrici dei rating annesse ad essa, per ciascuna di esse si eseguono le seguenti operazioni:
    \begin{enumerate}
        \item \textit{Alleno} i modelli ($MF$, $UserKnn$) con valori di deafult con il training set;
        \item valuto i risultati del modello sul validation set, se questi valori risultano migliori di quelli di base (dati dal $MostPop$), allora si procede ad un tuning dei parametri sul modello con quella matrice dei rating;
        \item Una volta concluso il tuning, si sceglie il modello migliore in accordo al validation set;
        \item Si va a confrontare la valutazione finale basata sul test set, con quella di base ottenutta con il $VAECF$.
        \item Tra tutte quelle che superano quest'ultimo passaggio scegliamo la migliore, una per ogni dataset.
    \end{enumerate}
\end{enumerate}

Il confronto dei risultati non è puramente matematico ma va a valutare il trand generale del metodo, se buona parte dei risultati sono migliori del bound si procede con il tuning, mentre se solo alcuni valori sono leggermente meglio li si lascia perdere.

\subsection{Esperimenti sulle matrici grezze combinate}
Per prima cosa si è proceduto a dividere in gruppi tutte le matrici dei rating aventi stesso metodo e stessa scala, si sono ottenuti tutti gruppi aventi 4 matrici, una per ogni \textit{espressione d'interesse}. Per ognuno di questi gruppi si è proceduto ad applicare i due metodi combinati descritti nel capitolo precedente. Per gli esperimenti si sono usati lo stesso training, validation e test set usati per gli esperimenti precedenti. La fase preliminare di esperimenti mirava a capire se questo metodo potesse fornire risultati migliori rispetto quelli di base dati da $MostPop$ e $VAECF$.

\section{Esperimenti con approccio content-based}
Per questo approccio non si è arrivati alla fase sperimentale in quanto dai primi test è risultato chiaro che non si avessero abbastanza informazioni sugli item, infatti i profili degli user erano molto simili tra loro e nonostante diverse modifiche, per ogni user venivano restituiti circa gli stessi item. Probabilmente le misure continue essendo disponibili per un numero limitato di prodotti hanno fatto si di essere poco influenti nella media finale, mentre la gerarchia prodotto e i nomi hanno fatto si di raccomandare i prodotti più popolari. Concludendo non si è voluto sviluppare ulteriormente l'approccio preferendo provare altro.

\section{Esperimenti con approccio next-basket}
Con questo approccio si è usato un dataset diverso e quindi non è possibile confrontare i risultati con quelli precedenti. Si sono andati a selezionare tutti gli user che avessero almeno 5 fatture, che ricordiamo in questo approccio vengono viste come transazioni, riducendo il numero di user rispettivamente per il dataset macchine da 254 a 224, per i ricambi da 319 a 257 e per il totale da 322 a 257, possiamo notare come siano stati eliminati un numero considerevole di user che avevo acquistato ricambi. L'nsieme delle ultime transazioni $\{b_{u}^{Bu}| \forall u \in U\}$ sono andate a formare rispettivamente randomicamente al 50\% validation e test set, mentre le restanti transizioni sono state a formare il training set. 
\newpage
Si avevano i seguenti hyper-parametri:
\begin{itemize}
    \item asimmetria $\alpha \in \{0, 0.25, 0.5, 0.75, 1\}$;
    \item località $q \in \{1, 5, 10, 50, 100, 1000\}$;
    \item finestra di recentezza $r \in \{1, 5, 25, 100, \infty \}$ .
\end{itemize}

Si è andato a testare ogni combinazione di hyper-parametri sul validation set, selezionati quelli migliori si è calcolata la valutazione finale del test set.